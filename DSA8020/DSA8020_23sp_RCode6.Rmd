---
title: "DSA 8020 R Session 6: Non-parametric Regression and Shrinkage Methods"
author: "Whitney"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  pdf_document:
    toc: true
    toc_depth: 3
    fig_width: 6
    fig_height: 5.5
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Non-parametric Regression: Motorcycle Accident Simulation Data

A data frame giving a series of measurements of head acceleration in a simulated motorcycle accident, used to test crash helmets.

* \texttt{times}: time in milliseconds after impact

* \texttt{accel}: head acceleration in $g$


*Data Source:* Silverman, B. W. (1985) Some aspects of the spline smoothing approach to non-parametric curve fitting. *Journal of the Royal Statistical Society series B* 47, 1â€“52.

### Load and plot the data 

```{r,message=FALSE}
library(MASS)
data(mcycle)
attach(mcycle)
plot(times, accel, pch = "*", cex = 1, las = 1,
     xlab = "Time (ms)", ylab = "Acceleration (g)")
```

### Linear and polynomial regression fits

```{r,message=FALSE}
rg <- range(times)
xg = seq(rg[1], rg[2], 0.1) # prediction grids

plot(times, accel, pch = "*", cex = 1, las = 1,
     xlab = "Time (ms)", ylab = "Acceleration (g)")
lmFit <- lm(accel ~ times, data = mcycle)
abline(lmFit, col = "red")
Cub.polyFit <- lm(accel ~ poly(times, 3), data = mcycle) 
Cub.polyPred <- predict(Cub.polyFit, data.frame(times = xg))
lines(xg, Cub.polyPred, col = "blue")
```

### Kernel regression

$\hat{f}(x)=\hat{\mathbb{E}}(Y|X=x)= \frac{\sum_{i=1}^{n}K_{h}(x-x_{i})y_{i}}{\sum_{i=1}^{n}K_{h}(x-x_{i})},$
where $K_{h}$ is a kernel with a bandwidth $h$.

```{r,message=FALSE}
KernFit <- with(mcycle, ksmooth(times, accel, kernel = "normal", bandwidth = 0.5))
KernFit2 <- with(mcycle, ksmooth(times, accel, kernel = "normal", bandwidth = 5))

plot(times, accel, pch = "*", cex = 1, las = 1,
     xlab = "Time (ms)", ylab = "Acceleration (g)")
lines(KernFit$x, KernFit$y, col = "darkgreen")
lines(KernFit2$x, KernFit2$y, col = "blue")
```

### Local Polynomial Regression Fitting (*loess*)

```{r,message=FALSE,warning=FALSE}
library(ggplot2)
plot <- ggplot(aes(x = times, y = accel), data = mcycle)
plot <- plot + geom_point()
(plot <- plot + geom_smooth(method = "loess", degree = 2, span = 0.25, se = TRUE))
```

### Regression Splines

```{r,message=FALSE,warning=FALSE}
library(splines)
RegSplineFit <- lm(accel ~ bs(times, df = 10), data = mcycle)
summary(RegSplineFit)
RegSplinePred <- predict(RegSplineFit, data.frame(times = xg))

plot(times, accel, pch = "*", cex = 1, las = 1,
     xlab = "Time (ms)", ylab = "Acceleration (g)")
lines(xg, RegSplinePred, col = "darkgreen")
```

### Generalized additive models

```{r,message=FALSE,warning=FALSE}
library(mgcv)
GAMFit <- gam(accel ~ s(times), data = mcycle)
summary(GAMFit)
GAMpred <- predict(GAMFit, data.frame(times = xg))
plot(times, accel, pch = "*", cex = 1, las = 1,
     xlab = "Time (ms)", ylab = "Acceleration (g)")
lines(xg, GAMpred, col = "red")
```

### Smoothing splines

```{r,message=FALSE,warning=FALSE}
library(fields)
SpFit <- sreg(times, accel)
summary(SpFit)
plot(SpFit, which = 3, col = "blue", pch = 16, las = 1)
SpPred <- predict(SpFit, xg)

plot(times, accel, pch = "*", cex = 1, las = 1,
     xlab = "Time (ms)", ylab = "Acceleration (g)")
lines(xg, SpPred , col = "blue")
```

### Comparing Regression spline/GAM/smoothing spline fits

```{r,message=FALSE,warning=FALSE}
plot(times, accel, pch = "*", cex = 1, las = 1,
     xlab = "Time (ms)", ylab = "Acceleration (g)")
lines(xg, RegSplinePred, col = "darkgreen")
lines(xg, GAMpred, col = "red")
lines(xg, SpPred, col = "blue")
legend("topleft", legend = c("Reg Spline", "GAM", "Smoothing Spline"),
       col = c("darkgreen", "red", "blue"), lty = 1, bty = "n")
```

### Generalized additive models for multiple predictors


```{r}
library(faraway)
gamod <- gam(sr ~ s(pop15) + s(pop75) + s(dpi) + s(ddpi), data = savings)

par(mfrow = c(2, 2), mar = c(4, 3.85, 0.8, 0.5))
plot(gamod, las = 1)
```


## Shrinkage Methods

The rest of this R session is largely based on the R lab: Ridge Regression and the Lasso of the book "Introduction to Statistical Learning with Applications in R" by *Gareth James*, *Daniela Witten*, *Trevor Hastie* and *Robert Tibshirani*.
We will use the `glmnet` package to perform ridge regression and
the lasso to predict `Salary` on the `Hitters` data.

### Ridge Regression

1. Data Setup

```{r,message=FALSE,warning=FALSE}
library(ISLR)
data(Hitters)
Hitters = na.omit(Hitters)
head(Hitters)
summary(Hitters)
library(glmnet)
X <- model.matrix(Salary ~ ., data = Hitters)[, -1] 
y <- Hitters$Salary
```

The `glmnet()` function has an alpha argument that determines what type
of model is fit. If `alpha = 0` then a ridge regression model is fit, and if `alpha = 1` then a lasso model is fit. We first fit a ridge regression model, which minimizes 
$$\sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2 +\color{blue}{\lambda\sum_{j=1}^{p}\beta_{j}^2},$$ where $\lambda\geq 0$ is a *tuning parameter* to be determined.

2. Fit Ridge Regression over a grid of $\lambda$ values

```{r,message=FALSE,warning=FALSE}
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0, lambda = grid)
```

3. Ridge Regression Coefficients

```{r,message=FALSE,warning=FALSE}
dim(coef(ridge.mod))
```

We expect the coefficient estimates to be much smaller, in terms of $\ell_2$ norm,
when a large value of $\lambda$ is used. 

```{r,message=FALSE,warning=FALSE}
ridge.mod$lambda[50] #Display 50th lambda value
coef(ridge.mod)[, 50] # Display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge.mod)[-1, 50]^2)) # Calculate l2 norm
```

In contrast, here are the coefficients when $\lambda = 705$, along with their $\ell_2$
norm. Note the much larger $l_2$ norm of the coefficients associated with this
smaller value of $\lambda$.


```{r,message=FALSE,warning=FALSE}
ridge.mod$lambda[60] #Display 60th lambda value
coef(ridge.mod)[, 60] # Display coefficients associated with 60th lambda value
sqrt(sum(coef(ridge.mod)[-1, 60]^2)) # Calculate l2 norm
```

We can use the `predict()` function for a number of purposes. For instance,
we can obtain the ridge regression coefficients for a new value of $\lambda$, say 50:

```{r,message=FALSE,warning=FALSE}
predict(ridge.mod, s = 50, type = "coefficients")[1:20, ]
```

4. Training/Testing

We now split the samples into a training set and a test set in order
to estimate the test error of ridge regression and later on the lasso.

```{r,message=FALSE,warning=FALSE}
set.seed(1)
train <- sample(1:nrow(X), nrow(X) / 2)
test <- (-train)
y.test <- y[test]

# Fit Ridge regression to the training data
ridge.mod <- glmnet(X[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
# Predcit the salary to the testing data with lambda = 4
ridge.pred <- predict(ridge.mod, s = 4, newx = X[test,])
# Calculate the Root Mean Square Error (RMSE)
sqrt(mean((ridge.pred - y.test)^2))
# Compute the RMSE for the intercept-only model
sqrt(mean((mean(y[train]) - y.test)^2))
# Change to a much larger lambda
ridge.pred <- predict(ridge.mod, s = 1e10, newx = X[test,])
sqrt(mean((ridge.pred - y.test)^2))
# Change lambda to 0
ridge.pred <- predict(ridge.mod, s = 0, newx = X[test,])
sqrt(mean((ridge.pred - y.test)^2))

lm(y ~ X, subset = train)
predict(ridge.mod, s = 0, type = "coefficients")[1:20,]
```


Instead of arbitrarily choosing $\lambda = 4$, it would be better to
use cross-validation (CV) to choose the tuning parameter $\lambda$. We can do this using
the built-in cross-validation function, `cv.glmnet()`. By default, the function
performs 10-fold cross-validation, though this can be changed using the
argument `folds`.

5. Cross-Validation (CV)

```{r,message=FALSE,warning=FALSE}
set.seed(1)
# Fit ridge regression model on training data
cv.out <- cv.glmnet(X[train,], y[train], alpha = 0) 
# Select lamda that minimizes training MSE
(bestLambda = cv.out$lambda.min) 

ridge.pred <- predict(ridge.mod, s = bestLambda, newx = X[test,])
sqrt(mean((ridge.pred - y.test)^2))

plot(cv.out) # Draw plot of training MSE as a function of lambda
```

Finally, we refit our ridge regression model on the full data set,
using the value of $\lambda$ chosen by cross-validation, and examine the coefficient estimates.

```{r,message=FALSE,warning=FALSE}
# Fit ridge regression model on full dataset
out <- glmnet(X, y, alpha = 0)
# Display coefficients using lambda chosen by CV
predict(out, type = "coefficients", s = bestLambda)[1:20,] 

lm(y ~ X, subset = train)
```

### The Lasso 

We saw that ridge regression with a wise choice of $\lambda$ can outperform least
squares as well as the null model on the Hitters data set. We now ask
whether the lasso, which minimizes 
$$\sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2 +\color{blue}{\lambda\sum_{j=1}^{p}|\beta_{j}|}$$
can yield either a more accurate or a more interpretable
model than ridge regression. In order to fit a lasso model, we once again
use the `glmnet()` function; however, this time we use the argument `alpha=1`.


```{r,message=FALSE,warning=FALSE}
# Fit lasso model on training data
lasso.mod <- glmnet(X[train,], y[train], alpha = 1, lambda = grid) 
# Draw plot of coefficients
plot(lasso.mod, las = 1)    
```

Notice that in the coefficient plot that depending on the choice of tuning
parameter, some of the coefficients are exactly equal to zero. We now
perform cross-validation and compute the associated test error:


```{r,message=FALSE,warning=FALSE}
set.seed(1)
# Fit lasso model on training data
cv.out <- cv.glmnet(X[train,], y[train], alpha = 1) 
# Draw plot of training MSE as a function of lambda
plot(cv.out) 
# Select lamda that minimizes training MSE
bestLambda <- cv.out$lambda.min 
# Use best lambda to predict test data
lasso.pred <- predict(lasso.mod, s = bestLambda, newx = X[test,]) 
# Calculate test RMSE
sqrt(mean((lasso.pred - y[test])^2)) 
```

This is substantially lower than the test set RMSE of the null model and of
least squares, and very similar to the test RMSE of ridge regression with $\lambda$
chosen by cross-validation.

However, the lasso has a substantial advantage over ridge regression in
that the resulting coefficient estimates are sparse. Here we see that 8 of
the 19 coefficient estimates are exactly zero:


```{r,message=FALSE,warning=FALSE}
# Fit lasso model on full dataset
out <- glmnet(X, y, alpha = 1, lambda = grid) 
# Display coefficients using lambda chosen by CV
(lasso.coef <- predict(out, type = "coefficients", s = bestLambda)[1:20,]) 
lasso.coef[lasso.coef != 0] # Display only non-zero coefficients
```


